# Topic: Parser & Building an Abstract Syntax Tree

**Course** : Formal Languages & Finite Automata

**Author** : Baxanean Constantin

***

## Theory

 Parsers are essential components in programming language processing. They analyze the structure of a sequence of tokens, typically generated by a lexer, and ensure it adheres to the syntax rules defined by a language or grammar. Parsers construct parse trees or abstract syntax trees (ASTs) that represent the hierarchical structure of code. They play a crucial role in interpreting, compiling, analyzing, and enhancing code editors. Parsers use formal grammar specifications to define syntax rules and perform error detection and recovery. Overall, parsers are fundamental tools for language processing, enabling efficient code understanding and processing.
***

## Objectives
- Get familiar with parsing, what it is and how it can be programmed
- Get familiar with the concept of AST

***

## Implementation description

The tokens dictionary defines different types of tokens along with their corresponding regular expressions.
Each token is represented by a key-value pair, where the key is the token name (e.g., "INT", "PLUS") and the value is the regular expression pattern.

```
tokens = {
    
    "INT" : r"[0-9]+",
    "PLUS" : r"\+",
    "MINUS" : r"\-",
    "DIVIDE" : r"\/",
    "MULTIPLY" : r"\*",
    "LPAREN" : r"\(",
    "RPAREN" : r"\)",
    "NEWLINE": r"\n | \r | \r \n",
    "WHITESPACE": r"[ ]+",
    "INVALID": r".+",

}
```

Lexer:

The Lexer class tokenizes the content of a file based on the defined tokens.
It reads the content from a file and uses regular expressions to find matches for the defined tokens.
The tokenize() method returns a list of tokens, excluding whitespace and newline tokens.
```
import re
import Tokens as Tokens


class Lexer:
    file = None
    content = ""
    tokens = "|".join(f"(?P<{name}>{regex})" for name, regex in Tokens.tokens.items())

    def __init__(self, file_name):
        self.file = open(file_name)
        self.content = self.file.read()

    def tokenize(self):
        matches = re.finditer(self.tokens, self.content)

        tokens = []
        for match in matches:
            token_name = match.lastgroup
            token_value = match.group(token_name)

            if token_name in ["WHITESPACE", "NEWLINE"]:
                continue

            if token_name == "INVALID":
                raise Exception(f"Invalid token '{token_value}'")

            tokens.append((token_name, token_value))

        return tokens
```
Parser:

The Parser class parses the tokens generated by the lexer and constructs a parse tree.
It takes a file path as input and initializes a lexer to tokenize the file content.
The parse() method starts the parsing process and constructs a parse tree rooted at a "PROGRAM" node.
The parse_add(), parse_multiply(), and parse_factor() methods recursively build the parse tree based on the grammar rules.
The show_ast() method prints the parse tree.
```
from Lexer import Lexer
from ParserTree import ParseTree


class Parser:
    def __init__(self, path):
        program = Lexer(path)
        self.tokens = program.tokenize()
        self.index = 0
        self.ast = None

    def parse(self):
        self.ast = ParseTree("PROGRAM")
        self.parse_add(self.ast)

    def parse_factor(self, parent_node):
        parse_node = ParseTree("SECTION")
        parent_node.children.append(parse_node)
        if self.index < len(self.tokens):
            if self.tokens[self.index][0] == "LPAREN":
                parse_node.children.append(ParseTree(self.tokens[self.index][0], self.tokens[self.index][1]))
                self.index += 1
                self.parse_add(parse_node)
                if self.index < len(self.tokens) and self.tokens[self.index][0] == "RPAREN":
                    parse_node.children.append(ParseTree(self.tokens[self.index][0], self.tokens[self.index][1]))
                    self.index += 1
            elif self.tokens[self.index][0] in ["INT"]:
                parse_node.children.append(ParseTree(self.tokens[self.index][0], self.tokens[self.index][1]))
                self.index += 1

    def parse_add(self, parent_node):
        parse_node = ParseTree("EXPRESSION")
        parent_node.children.append(parse_node)
        self.parse_multiply(parse_node)
        if self.index < len(self.tokens) and self.tokens[self.index][0] in ["PLUS", "MINUS"]:
            parse_node.children.append(ParseTree(self.tokens[self.index][0], self.tokens[self.index][1]))
            self.index += 1
            self.parse_multiply(parse_node)

    def parse_multiply(self, parent_node):
        parse_node = ParseTree("TERM")
        parent_node.children.append(parse_node)
        self.parse_factor(parse_node)
        if self.index < len(self.tokens) and self.tokens[self.index][0] in ["MULTIPLY", "DIVIDE"]:
            parse_node.children.append(ParseTree(self.tokens[self.index][0], self.tokens[self.index][1]))
            self.index += 1
            self.parse_factor(parse_node)

    def show_ast(self):
        print(self.ast)
```
ParserTree:

The ParseTree class represents a node in the parse tree.
Each node has a type (e.g., "PROGRAM", "SECTION") and can optionally have a value.
The children attribute stores the child nodes of a particular node.
The __str__() method recursively converts the parse tree into a string representation, showing the type and value of each node.

```
class ParseTree:
    def __init__(self, n_type, value=None, children=None):
        self.type = n_type
        self.value = value
        self.children = children or []

    def __str__(self, level=0):
        if self.value is None:
            ret = "\t" * level + self.type + "\n"
        else:
            ret = "\t" * level + self.type + ": " + str(self.value) + "\n"
        for child in self.children:
            ret += child.__str__(level + 1)
        return ret
```
And the main.py file: 
```
import os

print(os.getcwd())

from Parser import Parser

def main():
    file_path = r"C:\Users\User\Desktop\lab5 lfaf/test.txt"  # Replace with the path to your code file
    parser = Parser(file_path)
    parser.parse()
    parser.show_ast()

if __name__ == "__main__":
    main()


```
## Results
```
PROGRAM
        EXPRESSION
                TERM
                        SECTION
                                INT: 12
                PLUS: +
                TERM
                        SECTION
                                LPAREN: (
                                EXPRESSION
                                        TERM
                                                SECTION
                                                        INT: 34
                                        MINUS: -
                                        TERM
                                                SECTION
                                                        INT: 5
                                RPAREN: )

```

## Conclusion

In conclusion, the laboratory work on Parser and Building an Abstract Syntax Tree (AST) provides valuable insights into language processing and the essential components involved in analyzing and understanding code structure. Throughout the laboratory, we have covered the following key aspects:

Lexer: We implemented a lexer, which is responsible for tokenizing the code file based on defined tokens and regular expressions. The lexer helps break down the code into meaningful tokens for further processing.

Parser: The parser plays a crucial role in analyzing the tokenized code and constructing an Abstract Syntax Tree (AST). We implemented a parser that follows specific grammar rules to validate the code's syntax and create a structured representation of the code's hierarchical structure.

Abstract Syntax Tree (AST): The AST represents the syntactic structure of the code and captures the relationships between different elements. By building an AST, we gain a deeper understanding of the code's structure, which can be utilized for further analysis, interpretation, or compilation.

By working on the laboratory exercises related to parsers and ASTs, we have gained practical experience in implementing these fundamental components of language processing. We have learned how to tokenize code using a lexer, apply grammar rules to parse the tokens, and construct an AST to represent the code's structure.

Understanding parsers and building ASTs is valuable for various language-related tasks, such as code interpretation, compilation, static analysis, and development of code editing tools. It enhances our ability to analyze and process code efficiently, leading to more robust and intelligent software systems.
